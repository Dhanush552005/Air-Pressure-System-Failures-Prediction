# ğŸš› APS Failure Prediction â€” Scania Trucks Dataset

This project aims to predict **failures in the Air Pressure System (APS)** of Scania trucks using machine learning models.  
The dataset was provided by **Scania CV AB** as part of the **Industrial Challenge 2016** (IDA 2016).

---

## ğŸ“¦ Dataset Overview

**Dataset Name:** APS Failure and Operational Data for Scania Trucks  
**Source:** Scania CV AB, Sweden  
**License:** GNU General Public License v3.0  
**Link:** [https://archive.ics.uci.edu/ml/datasets/APS+Failure+at+Scania+Trucks+Data+Set](https://archive.ics.uci.edu/ml/datasets/APS+Failure+at+Scania+Trucks+Data+Set)

### ğŸ“Š Key Information
- **Instances:** 60,000 (train) + 16,000 (test)
- **Attributes:** 171 anonymized operational sensor readings
- **Positive class (1):** Trucks with APS component failure (~1.7%)
- **Negative class (0):** Trucks with other failures
- **Missing values:** Represented as `"na"`

---

## âš™ï¸ Problem Description

The task is a **binary classification**:
- **1:** APS component failure (positive)
- **0:** Other failure (negative)

Because the dataset is **highly imbalanced**, model evaluation must consider the **cost of misclassification**:

| Predicted \ True | Positive | Negative |
|------------------|-----------|-----------|
| **Positive** | â€” | Costâ‚ = 10 |
| **Negative** | Costâ‚‚ = 500 | â€” |

ğŸ’° **Total Cost = (10 Ã— False Positives) + (500 Ã— False Negatives)**

---

## ğŸ§¹ Data Preprocessing

1. **Loaded dataset** and replaced `"na"` with `NaN`.  
2. **Imputed missing values** using median values per feature.  
3. **Scaled data** using `StandardScaler` for model consistency.  
4. **Train-test split:** 80â€“20 ratio with stratified sampling.  
5. **Handled imbalance** using `class_weight='balanced'` where applicable.

---

## ğŸ§  Models Used

| Model | Library | Notes |
|--------|----------|--------|
| **Logistic Regression** | `sklearn.linear_model` | Interpretable baseline |
| **Random Forest** | `sklearn.ensemble` | Robust ensemble model |
| **Gradient Boosting** | `sklearn.ensemble` | Sequential boosting of weak learners |
| **XGBoost** | `xgboost` | Optimized gradient boosting algorithm |

---

## ğŸš€ Training & Evaluation

Each model was trained on the preprocessed dataset and evaluated on the test set using:
- **Accuracy**
- **Precision / Recall / F1-score**
- **Confusion Matrix**
- **Total Cost Metric**

---

## ğŸ“ˆ Model Performance Comparison

| Model | Accuracy | Precision (Pos) | Recall (Pos) | F1-Score (Pos) | FP | FN | **Total Cost** |
|--------|-----------|----------------|---------------|----------------|----|----|----------------|
| **Logistic Regression** | 0.974 | 0.38 | 0.81 | 0.51 | 943 | 133 | **â‚¹99,430** |
| **Random Forest** | 0.989 | 0.81 | 0.46 | 0.59 | 74 | 376 | **â‚¹193,740** |
| **XGBoost** | 0.990 | 0.95 | 0.43 | 0.59 | 15 | 399 | **â‚¹201,490** |

ğŸ§® **Cost Formula:**  
`Total Cost = (10 Ã— False Positives) + (500 Ã— False Negatives)`

---

### ğŸ” Observations
- **Logistic Regression** achieves the **lowest total cost**, even though its accuracy is slightly lower.  
- **XGBoost** and **Random Forest** show superior raw accuracy but incur **higher penalties** due to missed failures.  
- The **positive class (failures)** is small, so recall is critical â€” missing a failure costs 50Ã— more than a false alarm.

---

### ğŸ“Š Confusion Matrices

| Model | Confusion Matrix |
|--------|------------------|
| **Logistic Regression** | `[[40357, 943], [133, 567]]` |
| **Random Forest** | `[[41226, 74], [376, 324]]` |
| **XGBoost** | `[[41285, 15], [399, 301]]` |

---

## ğŸ§® Cost Analysis Visualization (Optional Code)

```python
import matplotlib.pyplot as plt

models = ['Logistic Regression', 'Random Forest', 'XGBoost']
costs = [99430, 193740, 201490]

plt.bar(models, costs)
plt.ylabel('Total Cost (Lower is Better)')
plt.title('Cost-based Comparison of Models')
plt.show()
